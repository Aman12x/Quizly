So we've been talking a lot about the neural net, but we haven't really gone into detail about what neural network is. So a neural network typically consists of, it contains three different types of layers. We have a input layer over here that takes input from our domain. So for example, the example that input may be image data transferred into numbers that we then push into the model. So that's the input layer that handles that. Then we have something called hidden layers. Now these are layers that are not the input or output layers. So these kind of hidden in the middle between those two. And they take input from one layer and pass the output to another layer. And you can have multiple hidden layers, chain one after the other. And that in fact, it is that that creates a deep learning model. And we have the output layer. So the output layer is the final layer that makes a prediction based on whatever was put under hidden layer and pass through all those hidden layers and then pass to the final output layer. So the perceptron is at its very core, almost like a single layer neural network with four main parameters. So we'll explain that in just a moment. But by putting all these perceptrons together, that is what creates a neural network. But let's have a look at what a perceptron looks like. So as I said, single layer neural network, four main parameters. We have the input values over here. We have the weights here. So this is those needs W's plus a bias term over here. We take some of these values and the weights and the biases, which is output, sorry, the inputs and weights and the biases, which is output from here. And then there is this activation function, which consumes that output and the sum outputs and outputs a different value, which is of course, then leads on to output. So this diagram here shows the forward propagation of information through a single neural network. When I say forward propagation, it's essentially the forward movement of information through a network. Here the information is defined by its inputs. We have x1, x2 all the way through to xm where m is just the number of features on number of input values in the network. And we of course take the sum of the multiplication of the inputs and their weights. So we multiply this x1 by w1, giving us something like this. And then we add on the, so we sum those, sorry. And then we add on the bias to the end of that, sorry, sorry. This calculation returns a single number. Depending so for each one of these values. So for x1, w1, we are going to output a final value here. So this will be, let's call it, just s1 for now. And then we would also, for example, s2 and all the way down to sm. These are all then fed into the activation function, sorry, go back to, I don't know, I said that wrong. So we go back because in this case, so this is to whoever's editing the video, in this case that's wrong, all of these sum together, so they do actually output a single value from the sum. So just go back to before, is there anything about having multiple values? So from all of these, this sum is actually going from one or x1, x2 all the way through to m and w1, 2 all the way through to m. All of that outputs a single value here. Okay, so our sum value, which we'll just call s for now. Okay, we don't actually use the notation s, but that's just what we will show here for now. And this sum value is fed into an activation function where it generates the apple, speak more of activation functions very soon. The bias value allows us to shift the activation regardless of the input, similar to the role of the constant c in a linear function. So the linear function is this, you may have seen it before. So it's this b or bias value is very similar to the c in this. And then we can almost, we can actually see this as, so m is our weights and x is our inputs. Okay. So we can represent this four propagation of a neuron mathematically using this. Okay. So we have our output, which is what this y hat at the end here, we use this hat to denote a prediction rather than a true value. We have the activation function, which we denote with this g. Again we'll speak about the activation function very soon. We have the sum from i equals one all the way through to n, which is equivalent to m here. We switch notation a bit quite often to be honest sometimes. And i is just, okay, which, which position are we in? So i equals one here, i equals two here and so on. And then we also just add the bias term after we've summed all of those weights and activations, impact activations, we have to bias onto the end of that. So it's a relatively simple formula. And what we tend to do in machine learning is actually use linear algebra to represent the linear combination of the inputs and weights in vector format. Okay. So this formula here, we're using the capitalized x and w. That is because x and w are vectors. Okay. They contain x1, x2 all the way through to xm. And w1, w2 all the way through to wn. And then we have this little t here. That means that this x rather than being a vector, we're actually going to transpose it. So that t means transpose into x1, x2 dot dot xn. Okay. And that's useful because we are going to perform a vector multiplication here between x and w. And to do that, we need, well, basically we need something that looks so the first shape must be something like this. And the second shape must be something like this. Okay. So that's why we're transposing to get that matchup in the end values on both of those shapes. So at its core, that is how a neural network for propagation step works in a single neuron. Of course, there is this g that we haven't spoken about yet. That's the activation function. So that's what we're going to cover next. So I hope that's been useful and I will see you in the next class.