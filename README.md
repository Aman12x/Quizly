# ğŸ“ Quizly

An interactive **Streamlit application** that transforms any uploaded video into a **summarized transcript** and generates **challenging quiz questions** with explanations â€” powered by **Whisper**, **Ollama (LLaMA 3.1)**, and **LangChain**.

---

## ğŸš€ Features
- ğŸ“¤ **Upload video** in common formats (MP4, AVI, MOV, etc.)
- ğŸ™ **Automatic audio extraction** from video
- ğŸ“ **Speech-to-text transcription** with Whisper
- ğŸ“„ **Smart summarization** using LLaMA 3.1 (Ollama)
- â“ **Quiz generation** â€” 5 challenging questions with explanations
- ğŸ“Š **Instant scoring** and result download

---

## ğŸ“¦ Tech Stack
- **[Streamlit](https://streamlit.io/)** â€” Interactive UI
- **[MoviePy](https://zulko.github.io/moviepy/)** â€” Audio extraction from video
- **[OpenAI Whisper](https://github.com/openai/whisper)** â€” Speech-to-text
- **[Ollama](https://ollama.ai/)** â€” Local LLaMA 3.1 model serving
- **[LangChain](https://python.langchain.com/)** â€” LLM orchestration

---

## ğŸ“‹ Prerequisites
- **Python 3.9+**
- **Ollama installed & running locally**  
  [Install Ollama â†’](https://ollama.ai/download)
- LLaMA 3.1 model pulled in Ollama:
  ```bash
  ollama pull llama3.1
  ```

---

## âš™ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/quiz-generator.git
   cd quiz-generator
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Start Ollama server**
   ```bash
   ollama serve
   ```

4. **Run the Streamlit app**
   ```bash
   streamlit run main.py
   ```

---

## ğŸ–¥ Usage
1. Open the app in your browser (default: `http://localhost:8501`)
2. Upload a video file
3. Wait for:
   - Audio extraction
   - Transcription
   - Summarization
   - Quiz generation
4. Take the quiz and view your score
5. Download your quiz results (JSON)

---

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ main.py                        # Streamlit UI & core logic
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ processing.py                   # Processing pipeline
â”œâ”€â”€ utils.py                        # Helper functions
â”œâ”€â”€ results_*.json                  # Example quiz outputs
â””â”€â”€ video_summary.ipynb             # Optional Jupyter notebook
```

---

## ğŸš€ Deployment Notes
- **Streamlit Cloud:** Will not work with local Ollama â€” deploy on a VM/Docker host with Ollama installed and exposed.
- **Docker Deployment:**
  - Build an image containing both Ollama & the app
  - Or use `docker-compose` to run them as separate services

---

## ğŸ›  Troubleshooting
- **`Connection refused to localhost:11434`** â†’ Ensure Ollama is running locally (`ollama serve`) and accessible on port 11434.
- **Model not found** â†’ Run `ollama pull llama3.1`
- **Deployment issues on Streamlit Cloud** â†’ Use a remote Ollama server or a hosted LLM API.

---

## ğŸ”® Future Improvements
- **Integrate [Tavily Web Search](https://tavily.com/)** to enrich quiz generation with **real-time context from the web**, enabling the LLM to incorporate the latest information and provide more accurate, up-to-date answers and explanations.

---

